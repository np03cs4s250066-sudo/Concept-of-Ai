{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMzA3inl5CPLtJfmoBKoZJj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**2 Naive Bayes Classifier for Email Classification.**"],"metadata":{"id":"2Xrd3N81lavK"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YHrgrbFkmsJu","executionInfo":{"status":"ok","timestamp":1769007008534,"user_tz":-345,"elapsed":2156,"user":{"displayName":"Seraj Haidar Rain","userId":"07907871550803193570"}},"outputId":"e1532ec1-df4f-4a1e-f568-0b1e3d042b2c"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["**Step -1- Load the Data:**"],"metadata":{"id":"qlgYyC07n0Em"}},{"cell_type":"code","source":["import pandas as pd\n","import re\n","import nltk\n","\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import classification_report, accuracy_score\n","\n","# Download stopwords\n","nltk.download('stopwords')\n","\n","# Load dataset (FIX FOR ParserError)\n","df = pd.read_csv(\n","    \"/content/spam_ham_dataset.csv\",\n","    engine=\"python\",          # ← REQUIRED\n","    on_bad_lines=\"skip\",      # ← REQUIRED\n","    index_col=0\n",")\n","\n","# Create label_num if missing\n","if 'label_num' not in df.columns:\n","    if 'label' in df.columns:\n","        df['label_num'] = df['label'].map({'ham': 0, 'spam': 1})\n","    elif 'Category' in df.columns:\n","        df['label_num'] = df['Category'].map({'ham': 0, 'spam': 1})\n","    elif 'v1' in df.columns:\n","        df['label_num'] = df['v1'].map({'ham': 0, 'spam': 1})\n","    else:\n","        raise ValueError(\"No valid label column found\")\n","\n","# Text Cleaning\n","length = len(df['text'])\n","\n","corpus = []\n","ps = PorterStemmer()\n","all_stopwords = stopwords.words('english')\n","\n","for i in range(length):\n","    text = str(df['text'].iloc[i])        # bytes-safe\n","    text = re.sub('[^a-zA-Z]', ' ', text)\n","    text = text.lower()\n","    text = text.split()\n","    text = [ps.stem(word) for word in text if word not in set(all_stopwords)]\n","    text = ' '.join(text)\n","    corpus.append(text)\n","\n","# Verify & remove \"subject\"\n","data_check = df.copy()\n","data_check['cleanText'] = corpus\n","data_check['cleanText'] = data_check['cleanText'].str.replace(\n","    'subject', '', regex=False\n",")\n","\n","# Feature & label\n","x = data_check['cleanText'].values\n","y = data_check['label_num'].values\n","\n","# Count Vectorization\n","cv = CountVectorizer()\n","x = cv.fit_transform(x).toarray()\n","\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    x, y, test_size=0.3, random_state=42\n",")\n","\n","# Naive Bayes Model\n","model = MultinomialNB()\n","model.fit(X_train, y_train)\n","\n","# Evaluation\n","y_pred = model.predict(X_test)\n","\n","print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n","print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VmXFojOMzv5U","executionInfo":{"status":"ok","timestamp":1769007032798,"user_tz":-345,"elapsed":24240,"user":{"displayName":"Seraj Haidar Rain","userId":"07907871550803193570"}},"outputId":"04dffb13-b047-4e03-93b9-e668609eb29f"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Accuracy: 0.9755154639175257\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","           0       0.99      0.98      0.98      1121\n","           1       0.95      0.96      0.96       431\n","\n","    accuracy                           0.98      1552\n","   macro avg       0.97      0.97      0.97      1552\n","weighted avg       0.98      0.98      0.98      1552\n","\n"]}]},{"cell_type":"markdown","source":["**PART A — Naive Bayes (IMDB Sentiment Analysis)**"],"metadata":{"id":"7s9syGb75-QI"}},{"cell_type":"markdown","source":["**1. Load & Preprocess IMDB Dataset**"],"metadata":{"id":"Bb2NfcMp6GD3"}},{"cell_type":"code","source":["# 2.2 Sentiment Analysis – IMDB Movie Review Dataset\n","# Import Libraries\n","import pandas as pd\n","import re\n","import nltk\n","\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","\n","\n","# Download NLTK Stopwords\n","nltk.download('stopwords')\n","\n","# Load Dataset (ParserError FIXED)\n","df = pd.read_csv(\n","    \"IMDB Dataset.csv\",\n","    engine=\"python\",        # required for broken quotes\n","    on_bad_lines=\"skip\"     # skip malformed rows\n",")\n","\n","# Encode Target Labels\n","df['sentiment'] = df['sentiment'].map({\n","    'positive': 1,\n","    'negative': 0\n","})\n","\n","# Text Preprocessing\n","ps = PorterStemmer()\n","stop_words = set(stopwords.words('english'))\n","\n","def preprocess_text(text):\n","    text = str(text).lower()                  # lowercase\n","    text = re.sub('[^a-zA-Z]', ' ', text)     # remove punctuation\n","    words = text.split()                      # tokenize\n","    words = [ps.stem(w) for w in words if w not in stop_words]  # stopwords + stemming\n","    return ' '.join(words)\n","\n","df['clean_review'] = df['review'].apply(preprocess_text)\n","\n","# Train-Test Split (80 / 20)\n","X = df['clean_review']\n","y = df['sentiment']\n","\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42\n",")\n","\n","# Bag of Words (CountVectorizer)\n","cv = CountVectorizer(max_features=5000)\n","\n","X_train_vec = cv.fit_transform(X_train)\n","X_test_vec = cv.transform(X_test)\n","\n","# Naive Bayes Model\n","model = MultinomialNB()\n","model.fit(X_train_vec, y_train)\n","\n","# Prediction & Evaluation\n","y_pred = model.predict(X_test_vec)\n","\n","print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n","print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n","print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QmvLUzX36BEJ","executionInfo":{"status":"ok","timestamp":1769007170936,"user_tz":-345,"elapsed":138102,"user":{"displayName":"Seraj Haidar Rain","userId":"07907871550803193570"}},"outputId":"04d3da2a-d503-447e-a817-71bf55a617a9"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Accuracy: 0.8494\n","\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","           0       0.84      0.85      0.85      4961\n","           1       0.85      0.84      0.85      5039\n","\n","    accuracy                           0.85     10000\n","   macro avg       0.85      0.85      0.85     10000\n","weighted avg       0.85      0.85      0.85     10000\n","\n","\n","Confusion Matrix:\n"," [[4239  722]\n"," [ 784 4255]]\n"]}]},{"cell_type":"markdown","source":["**PART B — Feature Selection (Wrapper Method: RFE)**\n","\n","**1. Load Breast Cancer Dataset**"],"metadata":{"id":"cwgPu8-r-q3m"}},{"cell_type":"code","source":["from sklearn.datasets import load_breast_cancer\n","\n","data = load_breast_cancer()\n","X = data.data\n","y = data.target\n","feature_names = data.feature_names\n"],"metadata":{"id":"nnZU_wVD-64H","executionInfo":{"status":"ok","timestamp":1769007205802,"user_tz":-345,"elapsed":25,"user":{"displayName":"Seraj Haidar Rain","userId":"07907871550803193570"}}},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":["**2. Train-Test Split**"],"metadata":{"id":"j7InpV9i-99f"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42\n",")\n"],"metadata":{"id":"coOGsK8n-_7j","executionInfo":{"status":"ok","timestamp":1769007208727,"user_tz":-345,"elapsed":71,"user":{"displayName":"Seraj Haidar Rain","userId":"07907871550803193570"}}},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":["**3. Recursive Feature Elimination (Top 5 Features)**"],"metadata":{"id":"omq6goEA_DOd"}},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.feature_selection import RFE\n","\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","model = LogisticRegression(max_iter=1000)\n","rfe = RFE(estimator=model, n_features_to_select=5)\n","\n","rfe.fit(X_train_scaled, y_train)\n","\n","selected_features = feature_names[rfe.support_]\n","print(\"Selected Features:\", selected_features)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pEqjFQn6_FBu","executionInfo":{"status":"ok","timestamp":1769007211114,"user_tz":-345,"elapsed":141,"user":{"displayName":"Seraj Haidar Rain","userId":"07907871550803193570"}},"outputId":"8f9ecc7d-4a83-4198-9287-fae5e760ffc5"},"execution_count":60,"outputs":[{"output_type":"stream","name":"stdout","text":["Selected Features: ['radius error' 'worst radius' 'worst texture' 'worst area'\n"," 'worst concave points']\n"]}]},{"cell_type":"markdown","source":["**4. Train Model with Selected Features**"],"metadata":{"id":"wp1KddBM_IjC"}},{"cell_type":"code","source":["X_train_rfe = rfe.transform(X_train)\n","X_test_rfe = rfe.transform(X_test)\n","\n","model.fit(X_train_rfe, y_train)\n","y_pred_rfe = model.predict(X_test_rfe)\n"],"metadata":{"id":"V6JQOn_N_KYd","executionInfo":{"status":"ok","timestamp":1769007214213,"user_tz":-345,"elapsed":25,"user":{"displayName":"Seraj Haidar Rain","userId":"07907871550803193570"}}},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":["**5. Evaluation (Selected vs All Features)**"],"metadata":{"id":"HlzdoW1A_Nlz"}},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n","\n","print(\"Accuracy:\", accuracy_score(y_test, y_pred_rfe))\n","print(\"Precision:\", precision_score(y_test, y_pred_rfe))\n","print(\"Recall:\", recall_score(y_test, y_pred_rfe))\n","print(\"F1:\", f1_score(y_test, y_pred_rfe))\n","print(\"ROC-AUC:\", roc_auc_score(y_test, model.predict_proba(X_test_rfe)[:,1]))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"czoLk4T5_PWB","executionInfo":{"status":"ok","timestamp":1769007216534,"user_tz":-345,"elapsed":28,"user":{"displayName":"Seraj Haidar Rain","userId":"07907871550803193570"}},"outputId":"d7fcd6fa-d87b-470b-c5f7-d0f491825de6"},"execution_count":62,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.956140350877193\n","Precision: 0.9583333333333334\n","Recall: 0.971830985915493\n","F1: 0.965034965034965\n","ROC-AUC: 0.990501146413364\n"]}]},{"cell_type":"markdown","source":["**6. Experiment: Different Feature Counts**"],"metadata":{"id":"Gpv3bx3__So9"}},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.feature_selection import RFE\n","from sklearn.metrics import accuracy_score\n","\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","for k in [3, 7]:\n","    rfe = RFE(\n","        estimator=LogisticRegression(max_iter=2000),\n","        n_features_to_select=k\n","    )\n","\n","    rfe.fit(X_train_scaled, y_train)\n","\n","    X_train_k = rfe.transform(X_train_scaled)\n","    X_test_k = rfe.transform(X_test_scaled)\n","\n","    model = LogisticRegression(max_iter=2000)\n","    model.fit(X_train_k, y_train)\n","\n","    y_pred_k = model.predict(X_test_k)\n","\n","    print(f\"Top {k} Features Accuracy:\", accuracy_score(y_test, y_pred_k))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6woAPn_n_U1p","executionInfo":{"status":"ok","timestamp":1769007219709,"user_tz":-345,"elapsed":804,"user":{"displayName":"Seraj Haidar Rain","userId":"07907871550803193570"}},"outputId":"bb1bbea2-e136-4170-fa3a-899fbcb3e1a8"},"execution_count":63,"outputs":[{"output_type":"stream","name":"stdout","text":["Top 3 Features Accuracy: 0.9649122807017544\n","Top 7 Features Accuracy: 0.9736842105263158\n"]}]}]}